# Dockerfile.debug
FROM vllm/vllm-openai:v0.11.0

# ====== Build arguments / environment ======
ARG HF_TOKEN
ARG VLLM_REF=main

ENV HF_TOKEN="${HF_TOKEN}" \
    HF_HUB_ENABLE_HF_TRANSFER=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    VLLM_DECODE_TIMINGS_DIR=/vllm-workspace/benchmarks/multi_turn/decode_timings

# ====== Base tools ======
RUN apt-get update && apt-get install -y --no-install-recommends \
    git wget ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# ====== Python dependencies ======
RUN python3 -m pip install --no-cache-dir -U \
    "huggingface-hub>=0.34.0,<1.0" \
    requests tqdm pandas

# ====== Bake model into image (/model) ======
RUN mkdir -p /model && python3 - <<'PY'
from huggingface_hub import snapshot_download
import os

repo = "K-intelligence/Midm-2.0-Base-Instruct"
snapshot_download(
    repo_id=repo,
    local_dir="/model",
    token=os.environ.get("HF_TOKEN"),
    local_dir_use_symlinks=False,
)
PY

# ====== Clone vLLM repo ======
ARG CACHEBUST=1
RUN git clone https://github.com/hyunnnchoi/vllm.git /vllm && \
    cd /vllm && \
    git fetch --all && \
    git checkout "${VLLM_REF}" || true

# ====== Install editable vLLM before runtime ======
WORKDIR /vllm
RUN python3 -m pip uninstall -y vllm || true && \
    python3 -m pip install --no-cache-dir -e ".[all]"

# Prepare directories for decode timing outputs
RUN mkdir -p /vllm-workspace/benchmarks/multi_turn/decode_timings

# ====== Default working directory for benchmarks ======
WORKDIR /vllm/benchmarks/multi_turn

# ====== Server command (entrypoint inherited from base image) ======
CMD ["--model","/model","--served-model-name","Midm-2.0-Base-Instruct","--dtype","auto","--gpu-memory-utilization","0.92","--api-key","dummy"]
