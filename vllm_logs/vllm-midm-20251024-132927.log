DEBUG 10-23 21:29:37 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-23 21:29:37 [__init__.py:34] Checking if TPU platform is available.
DEBUG 10-23 21:29:37 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-23 21:29:37 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:29:37 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-23 21:29:37 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-23 21:29:37 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-23 21:29:37 [__init__.py:127] Checking if XPU platform is available.
DEBUG 10-23 21:29:37 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-23 21:29:37 [__init__.py:153] Checking if CPU platform is available.
DEBUG 10-23 21:29:37 [__init__.py:175] Checking if Neuron platform is available.
DEBUG 10-23 21:29:37 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:29:37 [__init__.py:78] Confirmed CUDA platform is available.
INFO 10-23 21:29:37 [__init__.py:241] Automatically detected platform cuda.
DEBUG 10-23 21:29:38 [utils.py:168] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 10-23 21:29:38 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-23 21:29:38 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-23 21:29:38 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1)[0;0m INFO 10-23 21:29:39 [api_server.py:1805] vLLM API server version 0.10.1.1
[1;36m(APIServer pid=1)[0;0m INFO 10-23 21:29:39 [utils.py:326] non-default args: {'model': '/model', 'served_model_name': ['Midm-2.0-Base-Instruct'], 'tensor_parallel_size': 4, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='76db5579-f12c-4dab-9011-81b880db8541', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None)}
[1;36m(APIServer pid=1)[0;0m INFO 10-23 21:29:50 [__init__.py:711] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1)[0;0m INFO 10-23 21:29:50 [__init__.py:1750] Using max model len 32768
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:29:50 [arg_utils.py:1705] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:29:50 [arg_utils.py:1714] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:29:50 [parallel.py:335] Defaulting to use mp for distributed inference
[1;36m(APIServer pid=1)[0;0m INFO 10-23 21:29:50 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 10-23 21:29:59 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-23 21:29:59 [__init__.py:34] Checking if TPU platform is available.
DEBUG 10-23 21:29:59 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-23 21:29:59 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:29:59 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-23 21:29:59 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-23 21:29:59 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-23 21:29:59 [__init__.py:127] Checking if XPU platform is available.
DEBUG 10-23 21:29:59 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-23 21:29:59 [__init__.py:153] Checking if CPU platform is available.
DEBUG 10-23 21:29:59 [__init__.py:175] Checking if Neuron platform is available.
DEBUG 10-23 21:29:59 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:29:59 [__init__.py:78] Confirmed CUDA platform is available.
INFO 10-23 21:29:59 [__init__.py:241] Automatically detected platform cuda.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:30:01 [utils.py:746] Waiting for 1 local, 0 remote core engine proc(s) to connect.
[1;36m(EngineCore_0 pid=269)[0;0m INFO 10-23 21:30:01 [core.py:636] Waiting for init message from front-end.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:30:01 [utils.py:831] HELLO from local core engine process 0.
[1;36m(EngineCore_0 pid=269)[0;0m DEBUG 10-23 21:30:01 [core.py:644] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/4db3d52b-9211-4b99-85d4-2e52f9a8df7f'], outputs=['ipc:///tmp/e042cc4d-b7ee-4caf-a487-13e79b4f7008'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, 'data_parallel_size': 1})
[1;36m(EngineCore_0 pid=269)[0;0m DEBUG 10-23 21:30:01 [core.py:481] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_0 pid=269)[0;0m DEBUG 10-23 21:30:01 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_0 pid=269)[0;0m DEBUG 10-23 21:30:01 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_0 pid=269)[0;0m DEBUG 10-23 21:30:01 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_0 pid=269)[0;0m INFO 10-23 21:30:01 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='/model', speculative_config=None, tokenizer='/model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Midm-2.0-Base-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=269)[0;0m WARNING 10-23 21:30:01 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_0 pid=269)[0;0m DEBUG 10-23 21:30:01 [shm_broadcast.py:243] Binding to ipc:///tmp/5e18bef3-77bd-4e18-9511-29b09a2d7daf
[1;36m(EngineCore_0 pid=269)[0;0m INFO 10-23 21:30:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_77a8f670'), local_subscribe_addr='ipc:///tmp/5e18bef3-77bd-4e18-9511-29b09a2d7daf', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 10-23 21:30:09 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-23 21:30:09 [__init__.py:34] Checking if TPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-23 21:30:09 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-23 21:30:09 [__init__.py:127] Checking if XPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-23 21:30:09 [__init__.py:153] Checking if CPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:175] Checking if Neuron platform is available.
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-23 21:30:09 [__init__.py:34] Checking if TPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
INFO 10-23 21:30:09 [__init__.py:241] Automatically detected platform cuda.
DEBUG 10-23 21:30:09 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-23 21:30:09 [__init__.py:34] Checking if TPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 10-23 21:30:09 [__init__.py:34] Checking if TPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-23 21:30:09 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-23 21:30:09 [__init__.py:127] Checking if XPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-23 21:30:09 [__init__.py:153] Checking if CPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:175] Checking if Neuron platform is available.
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-23 21:30:09 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-23 21:30:09 [__init__.py:127] Checking if XPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-23 21:30:09 [__init__.py:153] Checking if CPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:175] Checking if Neuron platform is available.
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
INFO 10-23 21:30:09 [__init__.py:241] Automatically detected platform cuda.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
INFO 10-23 21:30:09 [__init__.py:241] Automatically detected platform cuda.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 10-23 21:30:09 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 10-23 21:30:09 [__init__.py:127] Checking if XPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 10-23 21:30:09 [__init__.py:153] Checking if CPU platform is available.
DEBUG 10-23 21:30:09 [__init__.py:175] Checking if Neuron platform is available.
DEBUG 10-23 21:30:09 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 10-23 21:30:09 [__init__.py:78] Confirmed CUDA platform is available.
INFO 10-23 21:30:09 [__init__.py:241] Automatically detected platform cuda.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:30:11 [utils.py:750] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 10-23 21:30:11 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-23 21:30:11 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-23 21:30:11 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-23 21:30:11 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-23 21:30:11 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-23 21:30:11 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-23 21:30:11 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-23 21:30:11 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-23 21:30:11 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-23 21:30:11 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 10-23 21:30:11 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 10-23 21:30:11 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 10-23 21:30:14 [decorators.py:139] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
DEBUG 10-23 21:30:15 [__init__.py:3073] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7eff41736c00>
DEBUG 10-23 21:30:15 [__init__.py:3885] enabled custom ops: Counter()
DEBUG 10-23 21:30:15 [__init__.py:3887] disabled custom ops: Counter()
[1;36m(VllmWorker TP1 pid=403)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:313] Connecting to ipc:///tmp/5e18bef3-77bd-4e18-9511-29b09a2d7daf
DEBUG 10-23 21:30:15 [__init__.py:3073] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6856276240>
DEBUG 10-23 21:30:15 [__init__.py:3885] enabled custom ops: Counter()
DEBUG 10-23 21:30:15 [__init__.py:3887] disabled custom ops: Counter()
[1;36m(VllmWorker TP2 pid=404)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:313] Connecting to ipc:///tmp/5e18bef3-77bd-4e18-9511-29b09a2d7daf
DEBUG 10-23 21:30:15 [__init__.py:3073] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd1d2760da0>
DEBUG 10-23 21:30:15 [__init__.py:3885] enabled custom ops: Counter()
DEBUG 10-23 21:30:15 [__init__.py:3887] disabled custom ops: Counter()
[1;36m(VllmWorker TP3 pid=405)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:313] Connecting to ipc:///tmp/5e18bef3-77bd-4e18-9511-29b09a2d7daf
[1;36m(VllmWorker TP1 pid=403)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:243] Binding to ipc:///tmp/469f5a0b-3b57-4342-a30f-489d576cda81
[1;36m(VllmWorker TP1 pid=403)[0;0m INFO 10-23 21:30:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_05429584'), local_subscribe_addr='ipc:///tmp/469f5a0b-3b57-4342-a30f-489d576cda81', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP2 pid=404)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:243] Binding to ipc:///tmp/4b249204-952f-4a1a-a4f7-c6ae075cc2d4
[1;36m(VllmWorker TP2 pid=404)[0;0m INFO 10-23 21:30:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3737ab2e'), local_subscribe_addr='ipc:///tmp/4b249204-952f-4a1a-a4f7-c6ae075cc2d4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP3 pid=405)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:243] Binding to ipc:///tmp/ef9426f4-a935-472e-9ae0-6350ac309f0a
[1;36m(VllmWorker TP3 pid=405)[0;0m INFO 10-23 21:30:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2a5ff7bb'), local_subscribe_addr='ipc:///tmp/ef9426f4-a935-472e-9ae0-6350ac309f0a', remote_subscribe_addr=None, remote_addr_ipv6=False)
DEBUG 10-23 21:30:15 [__init__.py:3073] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f893fed47a0>
DEBUG 10-23 21:30:15 [__init__.py:3885] enabled custom ops: Counter()
DEBUG 10-23 21:30:15 [__init__.py:3887] disabled custom ops: Counter()
[1;36m(VllmWorker TP0 pid=402)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:313] Connecting to ipc:///tmp/5e18bef3-77bd-4e18-9511-29b09a2d7daf
[1;36m(VllmWorker TP0 pid=402)[0;0m DEBUG 10-23 21:30:15 [shm_broadcast.py:243] Binding to ipc:///tmp/bd7d4389-9428-47dc-94b6-6bcb2a3be567
[1;36m(VllmWorker TP0 pid=402)[0;0m INFO 10-23 21:30:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e365039c'), local_subscribe_addr='ipc:///tmp/bd7d4389-9428-47dc-94b6-6bcb2a3be567', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP2 pid=404)[0;0m DEBUG 10-23 21:30:15 [parallel_state.py:976] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:53893 backend=nccl
[1;36m(VllmWorker TP0 pid=402)[0;0m DEBUG 10-23 21:30:15 [parallel_state.py:976] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:53893 backend=nccl
[1;36m(VllmWorker TP1 pid=403)[0;0m DEBUG 10-23 21:30:15 [parallel_state.py:976] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:53893 backend=nccl
[1;36m(VllmWorker TP3 pid=405)[0;0m DEBUG 10-23 21:30:15 [parallel_state.py:976] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:53893 backend=nccl
[1;36m(VllmWorker TP0 pid=402)[0;0m DEBUG 10-23 21:30:16 [parallel_state.py:1027] Detected 1 nodes in the distributed environment
[1;36m(VllmWorker TP3 pid=405)[0;0m DEBUG 10-23 21:30:16 [parallel_state.py:1027] Detected 1 nodes in the distributed environment
[1;36m(VllmWorker TP2 pid=404)[0;0m DEBUG 10-23 21:30:16 [parallel_state.py:1027] Detected 1 nodes in the distributed environment
[1;36m(VllmWorker TP1 pid=403)[0;0m DEBUG 10-23 21:30:16 [parallel_state.py:1027] Detected 1 nodes in the distributed environment
[1;36m(VllmWorker TP3 pid=405)[0;0m INFO 10-23 21:30:16 [__init__.py:1418] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP0 pid=402)[0;0m INFO 10-23 21:30:16 [__init__.py:1418] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP2 pid=404)[0;0m INFO 10-23 21:30:16 [__init__.py:1418] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP3 pid=405)[0;0m INFO 10-23 21:30:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP0 pid=402)[0;0m INFO 10-23 21:30:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP2 pid=404)[0;0m INFO 10-23 21:30:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP1 pid=403)[0;0m INFO 10-23 21:30:16 [__init__.py:1418] Found nccl from library libnccl.so.2
[1;36m(VllmWorker TP1 pid=403)[0;0m INFO 10-23 21:30:16 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker TP2 pid=404)[0;0m [1;36m(VllmWorker TP3 pid=405)[0;0m WARNING 10-23 21:30:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 10-23 21:30:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker TP1 pid=403)[0;0m WARNING 10-23 21:30:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker TP0 pid=402)[0;0m WARNING 10-23 21:30:16 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker TP0 pid=402)[0;0m DEBUG 10-23 21:30:16 [shm_broadcast.py:243] Binding to ipc:///tmp/331c5ba4-efab-4eae-9427-50ab0e2326ad
[1;36m(VllmWorker TP0 pid=402)[0;0m INFO 10-23 21:30:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_82721919'), local_subscribe_addr='ipc:///tmp/331c5ba4-efab-4eae-9427-50ab0e2326ad', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker TP2 pid=404)[0;0m DEBUG 10-23 21:30:16 [shm_broadcast.py:313] Connecting to ipc:///tmp/331c5ba4-efab-4eae-9427-50ab0e2326ad
[1;36m(VllmWorker TP3 pid=405)[0;0m DEBUG 10-23 21:30:16 [shm_broadcast.py:313] Connecting to ipc:///tmp/331c5ba4-efab-4eae-9427-50ab0e2326ad
[1;36m(VllmWorker TP1 pid=403)[0;0m DEBUG 10-23 21:30:16 [shm_broadcast.py:313] Connecting to ipc:///tmp/331c5ba4-efab-4eae-9427-50ab0e2326ad
[1;36m(VllmWorker TP0 pid=402)[0;0m INFO 10-23 21:30:16 [parallel_state.py:1134] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker TP2 pid=404)[0;0m INFO 10-23 21:30:16 [parallel_state.py:1134] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker TP3 pid=405)[0;0m INFO 10-23 21:30:16 [parallel_state.py:1134] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker TP1 pid=403)[0;0m INFO 10-23 21:30:16 [parallel_state.py:1134] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker TP0 pid=402)[0;0m INFO 10-23 21:30:16 [factory.py:50] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 76db5579-f12c-4dab-9011-81b880db8541
[1;36m(VllmWorker TP0 pid=402)[0;0m WARNING 10-23 21:30:16 [base.py:80] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[1;36m(VllmWorker TP0 pid=402)[0;0m [32;20m[2025-10-23 21:30:16,734] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP0 pid=402)[0;0m [32;20m[2025-10-23 21:30:16,735] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP0 pid=402)[0;0m [32;20m[2025-10-23 21:30:16,735] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP0 pid=402)[0;0m [32;20m[2025-10-23 21:30:16,736] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP0 pid=402)[0;0m [32;20m[2025-10-23 21:30:16,736] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 2, 128) [3m(vllm_adapter.py:156:lmcache.integration.vllm.vllm_adapter)[0m
[1;36m(VllmWorker TP0 pid=402)[0;0m [32;20m[2025-10-23 21:30:16,736] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:945:lmcache.v1.cache_engine)[0m
[1;36m(VllmWorker TP3 pid=405)[0;0m INFO 10-23 21:30:16 [factory.py:50] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 76db5579-f12c-4dab-9011-81b880db8541
[1;36m(VllmWorker TP3 pid=405)[0;0m WARNING 10-23 21:30:16 [base.py:80] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[1;36m(VllmWorker TP3 pid=405)[0;0m [32;20m[2025-10-23 21:30:16,749] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP2 pid=404)[0;0m INFO 10-23 21:30:16 [factory.py:50] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 76db5579-f12c-4dab-9011-81b880db8541
[1;36m(VllmWorker TP2 pid=404)[0;0m WARNING 10-23 21:30:16 [base.py:80] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[1;36m(VllmWorker TP2 pid=404)[0;0m [32;20m[2025-10-23 21:30:16,749] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP3 pid=405)[0;0m [32;20m[2025-10-23 21:30:16,749] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP3 pid=405)[0;0m [32;20m[2025-10-23 21:30:16,750] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP2 pid=404)[0;0m [32;20m[2025-10-23 21:30:16,750] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP2 pid=404)[0;0m [32;20m[2025-10-23 21:30:16,750] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP3 pid=405)[0;0m [32;20m[2025-10-23 21:30:16,750] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP3 pid=405)[0;0m [32;20m[2025-10-23 21:30:16,750] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 2, 128) [3m(vllm_adapter.py:156:lmcache.integration.vllm.vllm_adapter)[0m
[1;36m(VllmWorker TP1 pid=403)[0;0m INFO 10-23 21:30:16 [factory.py:50] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 76db5579-f12c-4dab-9011-81b880db8541
[1;36m(VllmWorker TP1 pid=403)[0;0m WARNING 10-23 21:30:16 [base.py:80] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[1;36m(VllmWorker TP1 pid=403)[0;0m [32;20m[2025-10-23 21:30:16,750] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP2 pid=404)[0;0m [32;20m[2025-10-23 21:30:16,751] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP3 pid=405)[0;0m [32;20m[2025-10-23 21:30:16,751] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:945:lmcache.v1.cache_engine)[0m
[1;36m(VllmWorker TP2 pid=404)[0;0m [32;20m[2025-10-23 21:30:16,751] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 2, 128) [3m(vllm_adapter.py:156:lmcache.integration.vllm.vllm_adapter)[0m
[1;36m(VllmWorker TP2 pid=404)[0;0m [32;20m[2025-10-23 21:30:16,751] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:945:lmcache.v1.cache_engine)[0m
[1;36m(VllmWorker TP1 pid=403)[0;0m [32;20m[2025-10-23 21:30:16,752] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP1 pid=403)[0;0m [32;20m[2025-10-23 21:30:16,752] LMCache INFO:[0m Loading LMCache config file /config_dir/lmcache_config.yaml [3m(utils.py:57:lmcache.integration.vllm.utils)[0m
[1;36m(VllmWorker TP1 pid=403)[0;0m [32;20m[2025-10-23 21:30:16,752] LMCache INFO:[0m LMCache Configuration: {'chunk_size': 256, 'cufile_buffer_size': None, 'local_cpu': True, 'max_local_cpu_size': '40.0 GB GB', 'local_disk': '/lmcache', 'max_local_disk_size': '100.0 GB GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'enable_blending': False, 'blend_recompute_ratio': 0.15, 'blend_min_tokens': 256, 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'error_handling': False, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'pre_caching_hash_algorithm': 'builtin', 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None} [3m(config.py:761:lmcache.v1.config)[0m
[1;36m(VllmWorker TP1 pid=403)[0;0m [32;20m[2025-10-23 21:30:16,752] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 2, 128) [3m(vllm_adapter.py:156:lmcache.integration.vllm.vllm_adapter)[0m
[1;36m(VllmWorker TP1 pid=403)[0;0m [32;20m[2025-10-23 21:30:16,753] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:945:lmcache.v1.cache_engine)[0m
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:30:21 [utils.py:750] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(APIServer pid=1)[0;0m DEBUG 10-23 21:30:31 [utils.py:750] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559] WorkerProc failed to start.
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559] Traceback (most recent call last):
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 533, in worker_main
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     worker = WorkerProc(*args, **kwargs)
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 401, in __init__
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     self.worker.init_device()
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 603, in init_device
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     self.worker.init_device()  # type: ignore
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 192, in init_device
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     init_worker_distributed_environment(self.vllm_config, self.rank,
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 612, in init_worker_distributed_environment
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     ensure_kv_transfer_initialized(vllm_config)
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_transfer_state.py", line 63, in ensure_kv_transfer_initialized
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     _KV_CONNECTOR_AGENT = KVConnectorFactory.create_connector(
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/factory.py", line 60, in create_connector
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     return connector_cls(config, role)
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py", line 27, in __init__
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     self._lmcache_engine = LMCacheConnectorV1Impl(vllm_config, role, self)
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/lmcache/integration/vllm/vllm_v1_adapter.py", line 342, in __init__
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     self.lmcache_engine = init_lmcache_engine(
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]                           ^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/lmcache/integration/vllm/vllm_adapter.py", line 213, in init_lmcache_engine
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     engine = LMCacheEngineBuilder.get_or_create(
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/lmcache/v1/cache_engine.py", line 947, in get_or_create
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     memory_allocator = cls._Create_memory_allocator(config, metadata)
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]   File "/usr/local/lib/python3.12/dist-packages/lmcache/v1/cache_engine.py", line 919, in _Create_memory_allocator
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]     return MixedMemoryAllocator(int(max_local_cpu_size * 1024**3))
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559]                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker TP2 pid=404)[0;0m ERROR 10-23 21:30:34 [multiproc_executor.py:559] ValueError: invalid literal for int() with base 10: '40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.0 GB40.
[1;36m(VllmWorker TP2 pid=404)[0;0m INFO 10-23 21:30:34 [multiproc_executor.py:520] Parent process exited, terminating worker
[1;36m(VllmWorker TP3 pid=405)[0;0m INFO 10-23 21:30:34 [multiproc_executor.py:520] Parent process exited, terminating worker
[1;36m(VllmWorker TP1 pid=403)[0;0m INFO 10-23 21:30:35 [multiproc_executor.py:520] Parent process exited, terminating worker
[1;36m(VllmWorker TP0 pid=402)[0;0m INFO 10-23 21:30:36 [multiproc_executor.py:520] Parent process exited, terminating worker
[rank0]:[W1023 21:30:37.714234866 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700] EngineCore failed to start.
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700] Traceback (most recent call last):
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 492, in __init__
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 80, in __init__
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]     self._init_executor()
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 96, in _init_executor
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_0 pid=269)[0;0m Process EngineCore_0:
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 472, in wait_for_ready
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700]     raise e from None
[1;36m(EngineCore_0 pid=269)[0;0m ERROR 10-23 21:30:38 [core.py:700] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(EngineCore_0 pid=269)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_0 pid=269)[0;0m     self.run()
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_0 pid=269)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 704, in run_engine_core
[1;36m(EngineCore_0 pid=269)[0;0m     raise e
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 691, in run_engine_core
[1;36m(EngineCore_0 pid=269)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=269)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 492, in __init__
[1;36m(EngineCore_0 pid=269)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 80, in __init__
[1;36m(EngineCore_0 pid=269)[0;0m     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_0 pid=269)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
[1;36m(EngineCore_0 pid=269)[0;0m     self._init_executor()
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 96, in _init_executor
[1;36m(EngineCore_0 pid=269)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_0 pid=269)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_0 pid=269)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 472, in wait_for_ready
[1;36m(EngineCore_0 pid=269)[0;0m     raise e from None
[1;36m(EngineCore_0 pid=269)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[1;36m(APIServer pid=1)[0;0m Traceback (most recent call last):
[1;36m(APIServer pid=1)[0;0m   File "<frozen runpy>", line 198, in _run_module_as_main
[1;36m(APIServer pid=1)[0;0m   File "<frozen runpy>", line 88, in _run_code
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1920, in <module>
[1;36m(APIServer pid=1)[0;0m     uvloop.run(run_server(args))
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 109, in run
[1;36m(APIServer pid=1)[0;0m     return __asyncio.run(
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
[1;36m(APIServer pid=1)[0;0m     return runner.run(main)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
[1;36m(APIServer pid=1)[0;0m     return self._loop.run_until_complete(task)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 61, in wrapper
[1;36m(APIServer pid=1)[0;0m     return await main
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1850, in run_server
[1;36m(APIServer pid=1)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1870, in run_server_worker
[1;36m(APIServer pid=1)[0;0m     async with build_async_engine_client(
[1;36m(APIServer pid=1)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 178, in build_async_engine_client
[1;36m(APIServer pid=1)[0;0m     async with build_async_engine_client_from_engine_args(
[1;36m(APIServer pid=1)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
[1;36m(APIServer pid=1)[0;0m     return await anext(self.gen)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 220, in build_async_engine_client_from_engine_args
[1;36m(APIServer pid=1)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[1;36m(APIServer pid=1)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 1557, in inner
[1;36m(APIServer pid=1)[0;0m     return fn(*args, **kwargs)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 174, in from_vllm_config
[1;36m(APIServer pid=1)[0;0m     return cls(
[1;36m(APIServer pid=1)[0;0m            ^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 120, in __init__
[1;36m(APIServer pid=1)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=1)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 102, in make_async_mp_client
[1;36m(APIServer pid=1)[0;0m     return AsyncMPClient(*client_args)
[1;36m(APIServer pid=1)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 767, in __init__
[1;36m(APIServer pid=1)[0;0m     super().__init__(
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 446, in __init__
[1;36m(APIServer pid=1)[0;0m     with launch_core_engines(vllm_config, executor_class,
[1;36m(APIServer pid=1)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=1)[0;0m   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
[1;36m(APIServer pid=1)[0;0m     next(self.gen)
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 706, in launch_core_engines
[1;36m(APIServer pid=1)[0;0m     wait_for_engine_startup(
[1;36m(APIServer pid=1)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
[1;36m(APIServer pid=1)[0;0m     raise RuntimeError("Engine core initialization failed. "
[1;36m(APIServer pid=1)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
